{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CivicSignal**\n",
    "This code supports an end-to-end machine learning workflow that combines unsupervised clustering (UMAP + HDBSCAN) with supervised learning (Decision Tree) to uncover meaningful user segments from Ontario 211 call data. The aim is to identify distinct population needs, behavioral patterns, and demographic trends in order to inform data-driven decision-making for public support services.\n",
    "-------------------------------------------------\n",
    "Author: \n",
    " 1. Guan-Wei Huang\n",
    " 2. Yu-Ting Lin\n",
    "-------------------------------------------------\n",
    "© 2025 CivicSignal. All rights reserved.\n",
    "\n",
    "This code was developed by the CivicSignal team as part of a data science initiative focused on public service optimization. Unauthorized use, reproduction, or distribution of this code or its components is prohibited without explicit permission from the authors. This code is intended for educational and non-commercial research purposes only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import umap\n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from math import pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2LrPvU54LPz"
   },
   "outputs": [],
   "source": [
    "# Load Datasets\n",
    "call_reports = pd.read_csv(\"./***.csv\", low_memory=False)\n",
    "needs = pd.read_csv(\"./***.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Data Preprocessing**\n",
    "1. Deduplication\n",
    "2. Merging\n",
    "3. Filtering\n",
    "4. Datetime Parsing\n",
    "5. Feature Engineering (date time)\n",
    "6. Missing Value Removal\n",
    "7. Category Mapping\n",
    "8. One-Hot Encoding (Need category)\n",
    "9. Concatenation of Features\n",
    "We performed data preprocessing to clean and prepare the call records. This included deduplication, missing value handling, categorical feature mapping, one-hot encoding, and the construction of a feature matrix incorporating need categories, demographics, and temporal information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4806,
     "status": "ok",
     "timestamp": 1745460246287,
     "user": {
      "displayName": "Alan Huang",
      "userId": "11194851463557684313"
     },
     "user_tz": 240
    },
    "id": "xqVMplPJ6BRw",
    "outputId": "01900297-ee59-4d6d-a13b-6dfd9c57abbb"
   },
   "outputs": [],
   "source": [
    "# Remove Duplicate CallReportNum\n",
    "needs_dedup = needs.sort_values(by='CallReportNum').drop_duplicates(subset='CallReportNum', keep='first')\n",
    "call_reports_dedup = call_reports.sort_values(by='CallReportNum').drop_duplicates(subset='CallReportNum', keep='first')\n",
    "\n",
    "# Merge Datasets, and Filter for Ontario\n",
    "df = pd.merge(call_reports_dedup, needs_dedup, on=\"CallReportNum\", how=\"inner\")\n",
    "df = df.query(\"StateProvince == 'ON'\").reset_index(drop=True)\n",
    "\n",
    "# Convert Date Column (Define and Apply Time Periods)\n",
    "df['CallDateAndTimeStart'] = pd.to_datetime(df['CallDateAndTimeStart'])\n",
    "\n",
    "def get_time_period(hour):\n",
    "    if 5 <= hour < 12:\n",
    "        return 'Morning'\n",
    "    elif 12 <= hour < 18:\n",
    "        return 'Afternoon'\n",
    "    else:\n",
    "        return 'Evening'\n",
    "\n",
    "df['TimePeriod'] = df['CallDateAndTimeStart'].dt.hour.apply(get_time_period)\n",
    "\n",
    "# Drop Incomplete Records\n",
    "df = df.dropna(subset=[\n",
    "    'AIRSNeedCategory',\n",
    "    'AgeCategory_ON',\n",
    "    'Gender',\n",
    "    'TimePeriod',\n",
    "    'CallLength'\n",
    "]).reset_index(drop=True)\n",
    "\n",
    "# Map Need Categories\n",
    "need_mapping = {\n",
    "    'Mental Health/Addictions': 'Mental Health',\n",
    "    'Mental Health/Substance Use Disorders': 'Mental Health',\n",
    "    'Income Support/Financial Assistance': 'Basic Needs',\n",
    "    'Utility Assistance': 'Basic Needs',\n",
    "    'Food/Meals': 'Basic Needs',\n",
    "    'Legal/Public Safety': 'Legal & Immigration',\n",
    "    'Citizenship/Immigration': 'Legal & Immigration',\n",
    "    'Other Government/Economic Services': 'Government Services',\n",
    "    'Consumer Services': 'Government Services',\n",
    "    'Community Services': 'Government Services',\n",
    "    'Individual/Family Services': 'Family Support',\n",
    "    'Information Services': 'Family Support'\n",
    "}\n",
    "df['NeedCategory_Grouped'] = df['AIRSNeedCategory'].map(need_mapping).fillna(df['AIRSNeedCategory'])\n",
    "\n",
    "#  Create One-Hot Encoded Features\n",
    "need_dummies = pd.get_dummies(df['NeedCategory_Grouped'], prefix='NeedCategory')\n",
    "ageCategory_dummies = pd.get_dummies(df['AgeCategory_ON'], prefix='AgeCategory_ON')\n",
    "gender_dummies = pd.get_dummies(df['Gender'], prefix='Gender')\n",
    "calllength_col = df['CallLength'].reset_index(drop=True)\n",
    "timeperiod_dummies = pd.get_dummies(df['TimePeriod'], prefix='TimePeriod')\n",
    "timeperiod_dummies.reset_index(drop=True, inplace=True)\n",
    "\n",
    "for dummy in [need_dummies, ageCategory_dummies, gender_dummies]:\n",
    "    dummy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Prepare Final Feature Set\n",
    "features = pd.concat([\n",
    "    need_dummies,\n",
    "    ageCategory_dummies,\n",
    "    gender_dummies,\n",
    "    timeperiod_dummies,\n",
    "    calllength_col\n",
    "], axis=1)\n",
    "\n",
    "#print(\"✅ df rows after filtering:\", len(df))\n",
    "#print(\"✅ features shape:\", features.shape)\n",
    "#print(features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Unsupervised Learning (UMAP + HDBSCAN)**\n",
    "1. Exploratory Data Analysis, EDA\n",
    "To identify natural groupings within the data, we used UMAP for nonlinear dimensionality reduction followed by HDBSCAN clustering. This approach allowed us to visualize and detect density-based clusters without requiring a predefined number of groups, while also labeling noise points effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 708
    },
    "executionInfo": {
     "elapsed": 1009237,
     "status": "ok",
     "timestamp": 1745468255551,
     "user": {
      "displayName": "Alan Huang",
      "userId": "11194851463557684313"
     },
     "user_tz": 240
    },
    "id": "FUfKnVsS6GLQ",
    "outputId": "cfd51907-bc4c-45c9-fadc-888006331b59"
   },
   "outputs": [],
   "source": [
    "# Initialize UMAP\n",
    "reducer = umap.UMAP(n_neighbors=150, min_dist=0.02, n_components=2, random_state=42)\n",
    "sample_idx = np.random.choice(len(features), size=50000, replace=False)  # Randomly Sample Data\n",
    "features_sample = features.iloc[sample_idx]\n",
    "\n",
    "# Apply UMAP\n",
    "embedding = reducer.fit_transform(features_sample)\n",
    "\n",
    "# Initialize and Fit HDBSCAN\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=600, min_samples=3)\n",
    "labels = clusterer.fit_predict(embedding)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(embedding[:, 0], embedding[:, 1], c=labels, cmap='tab10', s=50)\n",
    "plt.title('UMAP + HDBSCAN Clustering')\n",
    "plt.xlabel('UMAP 1')\n",
    "plt.ylabel('UMAP 2')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Post-Clustering Feature Profiling**\n",
    "1. Counted and ranked cluster sizes\n",
    "2. Selected top 10 most frequent clusters\n",
    "3. Calculated average feature values per cluster\n",
    "4. Standardized feature values for comparison\n",
    "5. Visualized feature strengths using heatmaps\n",
    "6. Analyzed age and gender patterns across clusters\n",
    "\n",
    "We analyzed the top 10 clusters by computing their average feature values and visualized their behavioral and demographic profiles. Z-score normalization was used for categorical features, and MinMax scaling was applied to call duration. These visualizations help identify unique patterns and dominant characteristics in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1745468288642,
     "user": {
      "displayName": "Alan Huang",
      "userId": "11194851463557684313"
     },
     "user_tz": 240
    },
    "id": "h8KueWDW_MnP",
    "outputId": "8feda5b8-21d7-4266-d332-b5efec7fe8ef"
   },
   "outputs": [],
   "source": [
    "# Count Cluster Labels\n",
    "label_counts = Counter(clusterer.labels_)\n",
    "cluster_distribution = pd.DataFrame.from_dict(label_counts, orient='index', columns=['Count'])\n",
    "cluster_distribution.sort_values(by='Count', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 716
    },
    "executionInfo": {
     "elapsed": 1727,
     "status": "ok",
     "timestamp": 1745485492435,
     "user": {
      "displayName": "Alan Huang",
      "userId": "11194851463557684313"
     },
     "user_tz": 240
    },
    "id": "2bbxfDZZB1ir",
    "outputId": "f185caf6-4601-40a1-fcd5-cf6f2dd7a9c4"
   },
   "outputs": [],
   "source": [
    "# Prepare Clustered Subset\n",
    "subset_idx = sample_idx\n",
    "subset_df = df.iloc[subset_idx].copy().reset_index(drop=True)\n",
    "subset_df['Cluster'] = clusterer.labels_\n",
    "\n",
    "# Rebuild Feature Subset\n",
    "features_sample = features.iloc[subset_idx].copy().reset_index(drop=True)\n",
    "subset_top = pd.concat([\n",
    "    features_sample.drop(columns='CallLength'),\n",
    "    subset_df['CallLength'],\n",
    "    subset_df['Cluster']\n",
    "], axis=1)\n",
    "\n",
    "# Select Top 10 Clusters\n",
    "top_clusters = subset_top['Cluster'].value_counts().head(10).index\n",
    "group_summary = subset_top[subset_top['Cluster'].isin(top_clusters)] \\\n",
    "                    .groupby('Cluster').mean().T\n",
    "\n",
    "# Filter Relevant Features\n",
    "filtered_features = [\n",
    "    col for col in group_summary.index\n",
    "    if not (col.startswith('AgeCategory_') or col.startswith('Gender_')) and col != 'CallLength'\n",
    "]\n",
    "\n",
    "# Scale Feature Averages (Z-Score)\n",
    "binary_scaled = pd.DataFrame(\n",
    "    StandardScaler().fit_transform(group_summary.loc[filtered_features]),\n",
    "    index=filtered_features,\n",
    "    columns=group_summary.columns\n",
    ")\n",
    "\n",
    "# Normalize CallLength (MinMax)\n",
    "calllength_row = group_summary.loc['CallLength'].values.reshape(-1, 1)\n",
    "calllength_scaled = MinMaxScaler().fit_transform(calllength_row).flatten()\n",
    "calllength_df = pd.DataFrame([calllength_scaled], index=['CallLength'], columns=group_summary.columns)\n",
    "\n",
    "# Combine Scaled Data\n",
    "final_summary = pd.concat([binary_scaled, calllength_df])\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(final_summary, cmap='coolwarm', center=0, annot=True, fmt=\".2f\")\n",
    "plt.title('Top 10 Cluster Feature Profiles (Z-score + MinMax for CallLength)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Scale Age Category Features\n",
    "age_features = [col for col in group_summary.index if col.startswith('AgeCategory')]\n",
    "age_scaled = pd.DataFrame(\n",
    "    StandardScaler().fit_transform(group_summary.loc[age_features]),\n",
    "    index=age_features,\n",
    "    columns=group_summary.columns\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(age_scaled, cmap='coolwarm', center=0, annot=True, fmt=\".2f\")\n",
    "plt.title('Cluster-wise AgeCategory Strengths (Z-score)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#  Scale Gender Features\n",
    "gender_features = [col for col in group_summary.index if col.startswith('Gender')]\n",
    "gender_scaled = pd.DataFrame(\n",
    "    StandardScaler().fit_transform(group_summary.loc[gender_features]),\n",
    "    index=gender_features,\n",
    "    columns=group_summary.columns\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.heatmap(gender_scaled, cmap='coolwarm', center=0, annot=True, fmt=\".2f\")\n",
    "plt.title('Cluster-wise Gender Strengths (Z-score)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **ML Decision Tree Classifier (Supervised Learning)**\n",
    "1. Assigned cluster labels to feature data\n",
    "2. Removed noise samples (unclustered points)\n",
    "3. Trained a decision tree to predict cluster membership\n",
    "4. Evaluated model performance\n",
    "5. Identified top features for differentiating clusters\n",
    "6. Visualized feature importance with a bar chart\n",
    "\n",
    "To interpret the unsupervised HDBSCAN clusters, we trained a Decision Tree classifier using cluster labels as the target variable. This enabled us to identify the most important features contributing to cluster separation and build an interpretable model for understanding behavioral differences across user segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ogq8OA7Cqvul"
   },
   "outputs": [],
   "source": [
    "# Assign Cluster Labels\n",
    "labels = clusterer.labels_\n",
    "df_clf = features_sample.copy().reset_index(drop=True)\n",
    "assert len(labels) == len(df_clf), f\"labels length: {len(labels)}, df_clf length: {len(df_clf)}\"\n",
    "df_clf['Cluster'] = labels\n",
    "\n",
    "# Remove Noise Labels (-1)\n",
    "df_clf = df_clf[df_clf['Cluster'] != -1].reset_index(drop=True)\n",
    "\n",
    "# Split Features and Target\n",
    "X = df_clf.drop(columns='Cluster')\n",
    "y = df_clf['Cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 525,
     "status": "ok",
     "timestamp": 1745468563874,
     "user": {
      "displayName": "Alan Huang",
      "userId": "11194851463557684313"
     },
     "user_tz": 240
    },
    "id": "f0iaue2Lqy2E",
    "outputId": "da40a6fb-f596-4574-960d-90e5d5522f41"
   },
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate Classifier\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Analyze Feature Importance\n",
    "feature_importance = pd.Series(clf.feature_importances_, index=X.columns)\n",
    "top_feature_importance = feature_importance.sort_values(ascending=False).head(15)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_feature_importance.plot(kind='barh')\n",
    "plt.title('Top Features for Cluster Differentiation')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Cluster Interpretation & Visualization**\n",
    "To interpret key behavioral traits of the most dominant clusters, we visualized the top 5 clusters using a radar chart. Each spoke represents a top discriminative feature, scaled using Min-Max normalization. This helps highlight which clusters show distinct strengths or needs across key categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "executionInfo": {
     "elapsed": 483,
     "status": "ok",
     "timestamp": 1745470243691,
     "user": {
      "displayName": "Alan Huang",
      "userId": "11194851463557684313"
     },
     "user_tz": 240
    },
    "id": "cu3Y47tYrtma",
    "outputId": "6d904cc7-c226-4e06-8d46-3f81d6c8134a"
   },
   "outputs": [],
   "source": [
    "# Get Top 5 Most Frequent Clusters (Excluding Noise)\n",
    "top_5_clusters = subset_top[subset_top['Cluster'] != -1]['Cluster'].value_counts().head(5).index.tolist()\n",
    "\n",
    "# Group by Cluster & Select Top Features\n",
    "radar_data = subset_top[subset_top['Cluster'].isin(top_5_clusters)] \\\n",
    "    .groupby('Cluster')[top_feature_names].mean()\n",
    "\n",
    "# Normalize Data for Radar Chart\n",
    "scaler = MinMaxScaler()\n",
    "radar_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(radar_data),\n",
    "    index=radar_data.index,\n",
    "    columns=radar_data.columns\n",
    ")\n",
    "\n",
    "# Prepare Radar Chart Angles\n",
    "labels = radar_scaled.columns.tolist()\n",
    "num_vars = len(labels)\n",
    "angles = [n / float(num_vars) * 2 * pi for n in range(num_vars)]\n",
    "angles += angles[:1] \n",
    "\n",
    "# Plot Radar Chart for Each Cluster\n",
    "plt.figure(figsize=(9, 9))\n",
    "for cluster in radar_scaled.index:\n",
    "    values = radar_scaled.loc[cluster].tolist()\n",
    "    values += values[:1] \n",
    "    plt.polar(angles, values, label=f'Cluster {cluster}', linewidth=2)\n",
    "\n",
    "plt.xticks(angles[:-1], labels, color='black', size=10)\n",
    "plt.title(\"Top Cluster Feature Strengths (Radar Chart)\", y=1.15)\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.35, 1.1))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1185,
     "status": "ok",
     "timestamp": 1745484555134,
     "user": {
      "displayName": "Alan Huang",
      "userId": "11194851463557684313"
     },
     "user_tz": 240
    },
    "id": "Xi2SakiU-1C6",
    "outputId": "4a2e308d-f477-426d-9c9f-b8a707972573"
   },
   "outputs": [],
   "source": [
    "# Export Clustered Records with Metadata\n",
    "export_df = subset_df[[\n",
    "    'CallReportNum',           \n",
    "    'CallDateAndTimeStart',\n",
    "    'CountryName',\n",
    "    'CityName',\n",
    "    'FSA',\n",
    "    'Level1Name',\n",
    "    'Level2Name',\n",
    "    'NeedCategory_Grouped',    \n",
    "    'AgeCategory_ON',\n",
    "    'Gender',\n",
    "    'TimePeriod',\n",
    "    'CallLength',\n",
    "    'Cluster'\n",
    "]]\n",
    "\n",
    "export_df.to_csv(\"clustered_call_records.csv\", index=False)\n",
    "print(\"clustered_call_records.csv with Cluster & Full Metadata\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP9RvW/dPbioMzq9q5T/cp9",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
